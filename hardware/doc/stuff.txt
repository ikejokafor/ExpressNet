SCNN: An Accelerator for Compressed-sparse
Convolutional Neural Networks

    This paper introduces the Sparse CNN (SCNN) accelerator architecture, which improves performance and energy efficiency by exploiting the zero-valued weights that stem
    from network pruning during training and zero-valued activations that arise from the common ReLU operator.

    On contemporary neural networks, SCNN can improve both performance and energy by a factor of 2.7× and 2.3×, respectively, over a comparably provisioned dense CNN accelerator.

    However, since the products generated by the multiplier array cannot be directly summed together, SCNN tracks the output coordinates associated with each multiplication and sends the coordinate
    and product to a scatter accumulator array for summation.
        so if you have 3x3 partial sums elements stored lineraly 0, 1, 4, 5, ..9 snd 2, 4, .. 8 corresponding elements must be summed 
        
    we employ a novel Cartesian product dataflow that exploits both weight and activation reuse while delivering only non-zero weights and activations to the multipliers. This dataflow performs an all-to-all
    multiply of non-zero weight and activation vector elements that can avoid any arithmetic based on zero-valued operands and achieve full multiplier utilization in steady-state.